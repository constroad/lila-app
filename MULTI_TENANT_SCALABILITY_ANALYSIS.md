# Multi-Tenant Scalability Analysis - Device ID Solution

**Project:** lila-app WhatsApp Integration
**Date:** 2026-02-06
**Status:** ğŸ”´ Critical Gap - Multi-Tenant Impact Assessment
**Related Document:** `DEVICE_ID_ROOT_CAUSE_ANALYSIS.md`

---

## ğŸ¯ EXECUTIVE SUMMARY

### The Gap

The `DEVICE_ID_ROOT_CAUSE_ANALYSIS.md` document provides excellent technical analysis of the Device ID problem but **does NOT address**:

1. âŒ Multi-tenant architecture implications
2. âŒ Concurrent session management at scale
3. âŒ Resource contention between tenants
4. âŒ Quota impact of proposed solutions
5. âŒ Scalability bottlenecks with 20-50 concurrent sessions

### The Reality

**lila-app is a MULTI-TENANT SaaS platform** where:
- Multiple companies share the same instance
- Each company can have MULTIPLE WhatsApp sessions
- System must handle 20-50 concurrent sessions
- Quotas and rate limits are PER-COMPANY
- Resource isolation is critical

### Critical Questions

The proposed solutions (Options A, B, C) must answer:

1. **Will makeInMemoryStore work with multiple sessions per tenant?**
2. **How do we prevent one tenant's corrupted session from affecting others?**
3. **What happens when 10 companies simultaneously trigger re-pairing?**
4. **Does device ID validation scale to 50 concurrent sessions?**
5. **Will migration scripts handle tenant isolation correctly?**

---

## ğŸ“Š CURRENT MULTI-TENANT ARCHITECTURE

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       lila-app Instance                      â”‚
â”‚  Single Node.js Process (PM2 managed)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Company A   â”‚  â”‚  Company B   â”‚  â”‚  Company C   â”‚     â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚     â”‚
â”‚  â”‚ Session 1    â”‚  â”‚ Session 1    â”‚  â”‚ Session 1    â”‚     â”‚
â”‚  â”‚ Session 2    â”‚  â”‚ Session 2    â”‚  â”‚ Session 2    â”‚     â”‚
â”‚  â”‚ Session 3    â”‚  â”‚              â”‚  â”‚ Session 3    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  All sharing:                                                â”‚
â”‚  - ConnectionManager singleton                               â”‚
â”‚  - Map<sessionPhone, socket>                                â”‚
â”‚  - MongoDB connection pool (5 connections)                   â”‚
â”‚  - Memory (4GB limit)                                        â”‚
â”‚  - Rate limiter Maps (in-memory)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Session Identification

**Key Insight:** Sessions are identified by **phone number**, NOT companyId.

```typescript
// Current structure
Map<string, socket>  // Key: phoneNumber (e.g., "51949376824")

// Problem: No direct link between session and company
// A phone number could theoretically be used by multiple companies
// (though business logic prevents this)
```

### Resource Sharing & Contention

**Shared Resources (All Tenants):**

1. **ConnectionManager singleton**
   - All sessions in same Map
   - Single cleanup/reconnect logic
   - No per-tenant isolation

2. **MongoDB Connection Pool**
   - 5 connections shared by ALL tenants
   - Quota checks compete for connections
   - Portal downtime affects all tenants

3. **In-Memory Rate Limiters**
   - Separate Map per company
   - Not distributed (single instance only)
   - Resets on restart

4. **File System**
   - WhatsApp sessions: `/data/sessions/{phoneNumber}/`
   - Company files: `/mnt/constroad-storage/companies/{companyId}/`
   - No quota on session storage

5. **Memory**
   - Node.js heap (~4GB max)
   - ~100MB per active session
   - No per-tenant memory limits

---

## ğŸ” IMPACT ANALYSIS: PROPOSED SOLUTIONS

### Option A: Complete Refactoring

**Proposed Changes:**
1. Remove manual socket.user reconstruction
2. Implement makeInMemoryStore
3. Refactor QR polling to SSE
4. Validate device ID in auth state

#### Multi-Tenant Impact Assessment

##### 1. makeInMemoryStore Implementation

**Original Proposal (Single-tenant assumption):**
```typescript
// src/whatsapp/baileys/store.ts
const stores = new Map<string, ReturnType<typeof makeInMemoryStore>>();

export function getOrCreateStore(sessionPhone: string) {
  let store = stores.get(sessionPhone);

  if (!store) {
    store = makeInMemoryStore({ logger });
    stores.set(sessionPhone, store);

    // Save every 30 seconds
    setInterval(() => {
      const data = JSON.stringify(store.toJSON());
      writeFile(storeFile, data); // âš ï¸ Missing error handling
    }, 30_000);
  }

  return store;
}
```

**Problems in Multi-Tenant Context:**

âŒ **Memory Leak Risk:**
- Each store maintains its own interval timer
- If session disconnects, interval continues
- With 50 sessions: 50 interval timers Ã— 30s = potential memory leak

âŒ **File I/O Bottleneck:**
- 50 stores Ã— write every 30s = ~1.67 writes/second
- Each write: ~100KB-5MB depending on store size
- No I/O queue management
- Concurrent writes can overwhelm disk

âŒ **No Per-Tenant Resource Limits:**
- Store can grow unbounded
- One tenant's large store doesn't affect their quota
- Store for inactive tenant never cleaned up

âŒ **Store File Location:**
```typescript
const storeFile = join('./data/sessions', sessionPhone, 'store.json');
```
- Stores in session directory (not company directory)
- No quota enforcement on session storage
- One company with many sessions = unlimited storage

**âœ… Improved Multi-Tenant Implementation:**

```typescript
// src/whatsapp/baileys/store.ts
import { EventEmitter } from 'events';

interface StoreMetadata {
  sessionPhone: string;
  companyId: string;
  createdAt: Date;
  lastAccessedAt: Date;
  sizeBytes: number;
}

class MultiTenantStoreManager extends EventEmitter {
  private stores = new Map<string, ReturnType<typeof makeInMemoryStore>>();
  private metadata = new Map<string, StoreMetadata>();
  private saveTimers = new Map<string, NodeJS.Timeout>();
  private readonly SAVE_INTERVAL_MS = 30000;
  private readonly MAX_STORE_SIZE_BYTES = 50 * 1024 * 1024; // 50MB per store
  private readonly INACTIVE_TIMEOUT_MS = 3600000; // 1 hour

  async getOrCreateStore(
    sessionPhone: string,
    companyId: string
  ): Promise<ReturnType<typeof makeInMemoryStore>> {
    let store = this.stores.get(sessionPhone);

    if (!store) {
      // Check company-level limits
      const companyStoreCount = this.getCompanyStoreCount(companyId);
      const maxStoresPerCompany = 10; // Configurable

      if (companyStoreCount >= maxStoresPerCompany) {
        throw new Error(
          `Company ${companyId} has reached maximum stores (${maxStoresPerCompany})`
        );
      }

      // Create store
      store = makeInMemoryStore({
        logger: logger as any
      });

      this.stores.set(sessionPhone, store);
      this.metadata.set(sessionPhone, {
        sessionPhone,
        companyId,
        createdAt: new Date(),
        lastAccessedAt: new Date(),
        sizeBytes: 0,
      });

      // Load persisted state
      await this.loadStoreState(sessionPhone);

      // Start save timer
      this.startSaveTimer(sessionPhone);

      logger.info(`Created store for ${sessionPhone} (company: ${companyId})`);
    }

    // Update last accessed
    const meta = this.metadata.get(sessionPhone);
    if (meta) {
      meta.lastAccessedAt = new Date();
    }

    return store;
  }

  private startSaveTimer(sessionPhone: string): void {
    const timer = setInterval(async () => {
      try {
        await this.saveStore(sessionPhone);
      } catch (error) {
        logger.error(`Failed to save store for ${sessionPhone}:`, error);
      }
    }, this.SAVE_INTERVAL_MS);

    this.saveTimers.set(sessionPhone, timer);
  }

  private async saveStore(sessionPhone: string): Promise<void> {
    const store = this.stores.get(sessionPhone);
    const meta = this.metadata.get(sessionPhone);

    if (!store || !meta) return;

    const storeFile = join(
      config.whatsapp.sessionDir,
      sessionPhone,
      'store.json'
    );

    try {
      const data = store.toJSON();
      const jsonString = JSON.stringify(data);
      const sizeBytes = Buffer.byteLength(jsonString, 'utf8');

      // Check size limit
      if (sizeBytes > this.MAX_STORE_SIZE_BYTES) {
        logger.warn(
          `Store for ${sessionPhone} exceeds max size (${sizeBytes} bytes), ` +
          `clearing old data`
        );
        // Clear old messages, keep only recent
        this.pruneStoreData(store);
      }

      // Atomic write with temp file
      const tempFile = `${storeFile}.tmp`;
      await fs.writeFile(tempFile, jsonString, 'utf8');
      await fs.rename(tempFile, storeFile);

      // Update metadata
      meta.sizeBytes = sizeBytes;

      logger.debug(
        `Saved store for ${sessionPhone} (${(sizeBytes / 1024).toFixed(2)} KB)`
      );
    } catch (error) {
      logger.error(`Failed to save store for ${sessionPhone}:`, error);
      throw error;
    }
  }

  private async loadStoreState(sessionPhone: string): Promise<void> {
    const storeFile = join(
      config.whatsapp.sessionDir,
      sessionPhone,
      'store.json'
    );

    if (await fs.pathExists(storeFile)) {
      try {
        const data = await fs.readJson(storeFile);
        const store = this.stores.get(sessionPhone);
        if (store) {
          store.fromJSON(data);
          logger.info(`Loaded store state for ${sessionPhone}`);
        }
      } catch (error) {
        logger.warn(`Failed to load store for ${sessionPhone}:`, error);
        // Continue with empty store
      }
    }
  }

  async removeStore(sessionPhone: string): Promise<void> {
    // Stop save timer
    const timer = this.saveTimers.get(sessionPhone);
    if (timer) {
      clearInterval(timer);
      this.saveTimers.delete(sessionPhone);
    }

    // Final save
    try {
      await this.saveStore(sessionPhone);
    } catch (error) {
      logger.warn(`Failed final save for ${sessionPhone}:`, error);
    }

    // Remove from memory
    this.stores.delete(sessionPhone);
    this.metadata.delete(sessionPhone);

    logger.info(`Removed store for ${sessionPhone}`);
  }

  private getCompanyStoreCount(companyId: string): number {
    let count = 0;
    for (const meta of this.metadata.values()) {
      if (meta.companyId === companyId) {
        count++;
      }
    }
    return count;
  }

  private pruneStoreData(store: ReturnType<typeof makeInMemoryStore>): void {
    // Keep only last 1000 messages per chat
    // Keep only last 7 days of data
    // This requires accessing store internals
    // Implementation depends on Baileys store structure
  }

  // Cleanup inactive stores
  async cleanupInactiveStores(): Promise<void> {
    const now = Date.now();
    const toRemove: string[] = [];

    for (const [sessionPhone, meta] of this.metadata.entries()) {
      const inactiveMs = now - meta.lastAccessedAt.getTime();

      if (inactiveMs > this.INACTIVE_TIMEOUT_MS) {
        toRemove.push(sessionPhone);
      }
    }

    for (const sessionPhone of toRemove) {
      logger.info(`Cleaning up inactive store: ${sessionPhone}`);
      await this.removeStore(sessionPhone);
    }
  }

  // Monitoring
  getStats() {
    const stats = {
      totalStores: this.stores.size,
      totalSizeBytes: 0,
      byCompany: new Map<string, { count: number; sizeBytes: number }>(),
    };

    for (const meta of this.metadata.values()) {
      stats.totalSizeBytes += meta.sizeBytes;

      const companyStat = stats.byCompany.get(meta.companyId) || {
        count: 0,
        sizeBytes: 0,
      };
      companyStat.count++;
      companyStat.sizeBytes += meta.sizeBytes;
      stats.byCompany.set(meta.companyId, companyStat);
    }

    return stats;
  }
}

// Singleton instance
const storeManager = new MultiTenantStoreManager();

// Cleanup every hour
setInterval(() => {
  storeManager.cleanupInactiveStores().catch(error => {
    logger.error('Failed to cleanup inactive stores:', error);
  });
}, 3600000);

export default storeManager;
```

**Benefits:**
- âœ… Per-company store limits
- âœ… Automatic cleanup of inactive stores
- âœ… Size limits per store (50MB)
- âœ… Atomic writes (no corruption)
- âœ… Monitoring and stats
- âœ… Proper timer cleanup

**Complexity:** Medium-High
**Risk:** Low (well-contained)

---

##### 2. Device ID Validation at Scale

**Original Proposal:**
```typescript
if (connection === 'open') {
  if (!socket.user) {
    // Clear session and reconnect
    await this.backupAndResetAuthState(sessionPhone, sessionDir);
    this.cleanupSession(sessionPhone, { clearQr: true });
    return;
  }

  const deviceId = extractDeviceId(socket.user.id);
  if (deviceId > 10) {
    // Clear session and reconnect
    await this.backupAndResetAuthState(sessionPhone, sessionDir);
    this.cleanupSession(sessionPhone, { clearQr: true });
    return;
  }
}
```

**Multi-Tenant Concerns:**

âŒ **Cascading Failures:**
- If 10 companies have corrupted sessions (device ID > 10)
- All trigger backup + clear simultaneously
- 10Ã— file I/O operations at once
- 10Ã— reconnection attempts simultaneously
- Can overwhelm disk and network

âŒ **No Rate Limiting on Validation:**
- Validation happens on every connection.open
- No throttling for repeated failures
- One misbehaving session can spam logs

âŒ **Backup Storm:**
```typescript
await this.backupAndResetAuthState(sessionPhone, sessionDir);
// Each backup: 5-10 MB copied
// 10 simultaneous: 50-100 MB I/O burst
// No queue, no limit
```

**âœ… Improved Multi-Tenant Implementation:**

```typescript
class DeviceIdValidator {
  private validationInProgress = new Set<string>();
  private lastValidationAttempt = new Map<string, number>();
  private readonly VALIDATION_COOLDOWN_MS = 60000; // 1 minute

  async validateAndHandle(
    socket: any,
    sessionPhone: string,
    companyId: string
  ): Promise<{ valid: boolean; action: string }> {
    // Prevent concurrent validation for same session
    if (this.validationInProgress.has(sessionPhone)) {
      logger.debug(`Validation already in progress for ${sessionPhone}`);
      return { valid: false, action: 'in_progress' };
    }

    // Rate limit validation attempts
    const lastAttempt = this.lastValidationAttempt.get(sessionPhone);
    if (lastAttempt && Date.now() - lastAttempt < this.VALIDATION_COOLDOWN_MS) {
      logger.debug(`Validation cooldown active for ${sessionPhone}`);
      return { valid: false, action: 'cooldown' };
    }

    this.validationInProgress.add(sessionPhone);
    this.lastValidationAttempt.set(sessionPhone, Date.now());

    try {
      // Check 1: socket.user exists
      if (!socket.user) {
        logger.error(
          `[${companyId}] socket.user undefined for ${sessionPhone}`
        );

        await this.handleInvalidSession(
          sessionPhone,
          companyId,
          'undefined_socket_user'
        );

        return { valid: false, action: 'cleared_undefined_user' };
      }

      // Check 2: Device ID is valid
      const deviceIdMatch = socket.user.id.match(/:(\d+)@/);
      if (!deviceIdMatch) {
        logger.error(
          `[${companyId}] Invalid ID format for ${sessionPhone}: ${socket.user.id}`
        );

        await this.handleInvalidSession(
          sessionPhone,
          companyId,
          'invalid_id_format'
        );

        return { valid: false, action: 'cleared_invalid_format' };
      }

      const deviceId = parseInt(deviceIdMatch[1], 10);

      // Check 3: Device ID in valid range
      if (deviceId > 10) {
        logger.error(
          `[${companyId}] Invalid device ID ${deviceId} for ${sessionPhone}`
        );

        await this.handleInvalidSession(
          sessionPhone,
          companyId,
          'high_device_id',
          { deviceId }
        );

        return { valid: false, action: 'cleared_high_device_id' };
      }

      // Success
      logger.info(
        `[${companyId}] Valid device ID ${deviceId} for ${sessionPhone}`
      );

      return { valid: true, action: 'validated' };
    } finally {
      this.validationInProgress.delete(sessionPhone);
    }
  }

  private async handleInvalidSession(
    sessionPhone: string,
    companyId: string,
    reason: string,
    metadata?: Record<string, any>
  ): Promise<void> {
    // Emit event for monitoring
    this.emit('invalid_session', {
      sessionPhone,
      companyId,
      reason,
      metadata,
      timestamp: new Date(),
    });

    // Increment company-level metric
    await this.incrementInvalidSessionMetric(companyId, reason);

    // Queue backup and clear (don't block)
    await backupQueue.enqueue({
      sessionPhone,
      companyId,
      reason,
      timestamp: Date.now(),
    });
  }

  private async incrementInvalidSessionMetric(
    companyId: string,
    reason: string
  ): Promise<void> {
    // Could store in Redis or MongoDB
    // For now, in-memory counter
    const key = `${companyId}:${reason}`;
    const current = this.invalidSessionCounts.get(key) || 0;
    this.invalidSessionCounts.set(key, current + 1);

    // Alert if threshold exceeded
    if (current + 1 >= 5) {
      logger.warn(
        `[${companyId}] High invalid session count for reason: ${reason} (${current + 1})`
      );
    }
  }

  // Stats for monitoring
  getStats() {
    return {
      validationInProgress: this.validationInProgress.size,
      lastValidations: this.lastValidationAttempt.size,
      invalidCounts: Object.fromEntries(this.invalidSessionCounts),
    };
  }
}
```

**Benefits:**
- âœ… Prevents validation storms
- âœ… Rate limits repeated failures
- âœ… Company-scoped metrics
- âœ… Non-blocking backup queue
- âœ… Monitoring hooks

---

##### 3. Migration Script Multi-Tenant Safety

**Original Proposal:**
```typescript
async function migrateSession(sessionPhone: string, deviceId: number) {
  // Backup
  const backupPath = await backupSession(sessionPhone);

  // Clear
  await clearSession(sessionPhone);

  console.log(`âš ï¸ USER ACTION REQUIRED: Re-scan QR code for ${sessionPhone}`);
}
```

**Multi-Tenant Concerns:**

âŒ **No Company Identification:**
- Migration script doesn't know which company owns which session
- Can't send targeted notifications
- Can't track migration status per company

âŒ **No Rate Limiting:**
- Could clear 50 sessions simultaneously
- No throttling of backups
- Can overwhelm disk I/O

âŒ **No Rollback Per Company:**
- If one company's migration fails, affects all
- No granular control

**âœ… Improved Multi-Tenant Migration:**

```typescript
// scripts/migrate-corrupted-sessions-multitenant.ts
import fs from 'fs-extra';
import path from 'path';
import { config } from '../src/config/environment.js';
import readline from 'readline';

interface SessionInfo {
  phoneNumber: string;
  companyId: string | null;
  deviceId: number;
  status: 'healthy' | 'corrupted' | 'unknown';
  credsPath: string;
}

interface CompanyMigrationReport {
  companyId: string;
  totalSessions: number;
  corruptedSessions: number;
  healthySessions: number;
  sessionsToMigrate: SessionInfo[];
}

async function identifySessionOwner(
  sessionPhone: string
): Promise<string | null> {
  // Option 1: Query MongoDB for company that owns this phone
  // (if there's a PhoneNumber â†’ Company mapping)

  // Option 2: Check API usage logs
  // (find which companyId used this session recently)

  // Option 3: Manual mapping file
  const mappingFile = './session-company-mapping.json';
  if (await fs.pathExists(mappingFile)) {
    const mapping = await fs.readJson(mappingFile);
    return mapping[sessionPhone] || null;
  }

  return null;
}

async function analyzeAllSessions(): Promise<Map<string, CompanyMigrationReport>> {
  const baseDir = config.whatsapp.sessionDir;
  const entries = await fs.readdir(baseDir);

  const sessionsByCompany = new Map<string, CompanyMigrationReport>();
  const orphanedSessions: SessionInfo[] = [];

  for (const entry of entries) {
    if (entry === 'backups' || entry === 'cleared') continue;

    const fullPath = path.join(baseDir, entry);
    const stat = await fs.stat(fullPath);

    if (!stat.isDirectory()) continue;

    const credsPath = path.join(fullPath, 'creds.json');
    if (!await fs.pathExists(credsPath)) {
      continue;
    }

    try {
      const creds = await fs.readJson(credsPath);

      if (!creds?.me?.id) {
        continue;
      }

      const deviceIdMatch = creds.me.id.match(/:(\d+)@/);
      if (!deviceIdMatch) {
        continue;
      }

      const deviceId = parseInt(deviceIdMatch[1], 10);
      const companyId = await identifySessionOwner(entry);

      const sessionInfo: SessionInfo = {
        phoneNumber: entry,
        companyId,
        deviceId,
        status: deviceId > 10 ? 'corrupted' : 'healthy',
        credsPath,
      };

      if (companyId) {
        let companyReport = sessionsByCompany.get(companyId);

        if (!companyReport) {
          companyReport = {
            companyId,
            totalSessions: 0,
            corruptedSessions: 0,
            healthySessions: 0,
            sessionsToMigrate: [],
          };
          sessionsByCompany.set(companyId, companyReport);
        }

        companyReport.totalSessions++;

        if (sessionInfo.status === 'corrupted') {
          companyReport.corruptedSessions++;
          companyReport.sessionsToMigrate.push(sessionInfo);
        } else {
          companyReport.healthySessions++;
        }
      } else {
        // Orphaned session (no known owner)
        orphanedSessions.push(sessionInfo);
      }
    } catch (error) {
      console.error(`Error analyzing ${entry}:`, error);
    }
  }

  // Handle orphaned sessions
  if (orphanedSessions.length > 0) {
    sessionsByCompany.set('_ORPHANED_', {
      companyId: '_ORPHANED_',
      totalSessions: orphanedSessions.length,
      corruptedSessions: orphanedSessions.filter(s => s.status === 'corrupted').length,
      healthySessions: orphanedSessions.filter(s => s.status === 'healthy').length,
      sessionsToMigrate: orphanedSessions.filter(s => s.status === 'corrupted'),
    });
  }

  return sessionsByCompany;
}

async function migrateCompany(
  companyReport: CompanyMigrationReport,
  dryRun: boolean
): Promise<void> {
  console.log(`\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”`);
  console.log(`â”‚ Company: ${companyReport.companyId.padEnd(40)} â”‚`);
  console.log(`â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤`);
  console.log(`â”‚ Total Sessions: ${String(companyReport.totalSessions).padStart(28)} â”‚`);
  console.log(`â”‚ Corrupted: ${String(companyReport.corruptedSessions).padStart(33)} â”‚`);
  console.log(`â”‚ Healthy: ${String(companyReport.healthySessions).padStart(35)} â”‚`);
  console.log(`â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`);

  if (companyReport.corruptedSessions === 0) {
    console.log(`âœ… No corrupted sessions for this company\n`);
    return;
  }

  for (const session of companyReport.sessionsToMigrate) {
    console.log(`  ğŸ“± ${session.phoneNumber} (Device ID: ${session.deviceId})`);

    if (dryRun) {
      console.log(`     [DRY RUN] Would backup and clear`);
    } else {
      try {
        // Backup
        const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
        const backupDir = path.join(
          config.whatsapp.sessionDir,
          'backups',
          session.phoneNumber,
          `migration-multitenant-${timestamp}`
        );

        await fs.ensureDir(backupDir);
        await fs.copy(
          path.dirname(session.credsPath),
          backupDir
        );

        console.log(`     âœ… Backed up to: ${backupDir}`);

        // Clear
        await fs.remove(path.dirname(session.credsPath));
        console.log(`     âœ… Cleared session`);

        // Log for company notification
        await logMigrationForCompany(
          companyReport.companyId,
          session.phoneNumber,
          backupDir
        );
      } catch (error) {
        console.error(`     âŒ Error:`, error);
      }
    }
  }

  console.log('');
}

async function logMigrationForCompany(
  companyId: string,
  phoneNumber: string,
  backupPath: string
): Promise<void> {
  const logFile = './migration-log.json';

  let log: any = {};
  if (await fs.pathExists(logFile)) {
    log = await fs.readJson(logFile);
  }

  if (!log[companyId]) {
    log[companyId] = {
      migratedSessions: [],
      timestamp: new Date().toISOString(),
    };
  }

  log[companyId].migratedSessions.push({
    phoneNumber,
    backupPath,
    migratedAt: new Date().toISOString(),
  });

  await fs.writeJson(logFile, log, { spaces: 2 });
}

async function main() {
  console.log('=== MULTI-TENANT CORRUPTED SESSION MIGRATION ===\n');

  // Step 1: Analyze all sessions by company
  console.log('ğŸ“Š Analyzing all sessions...\n');
  const sessionsByCompany = await analyzeAllSessions();

  // Step 2: Display summary
  console.log('=== SUMMARY BY COMPANY ===\n');

  let totalCorrupted = 0;
  for (const report of sessionsByCompany.values()) {
    if (report.corruptedSessions > 0) {
      console.log(`âš ï¸  ${report.companyId}: ${report.corruptedSessions} corrupted`);
      totalCorrupted += report.corruptedSessions;
    }
  }

  console.log(`\nğŸ“Š Total corrupted sessions across all companies: ${totalCorrupted}\n`);

  if (totalCorrupted === 0) {
    console.log('âœ… No corrupted sessions found. Migration not needed.');
    process.exit(0);
  }

  // Step 3: Confirm
  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout,
  });

  const dryRunAnswer = await new Promise<string>(resolve => {
    rl.question('Run in DRY RUN mode first? (Y/n): ', resolve);
  });
  const dryRun = dryRunAnswer.toLowerCase() !== 'n';

  if (!dryRun) {
    const confirmAnswer = await new Promise<string>(resolve => {
      rl.question(
        'âš ï¸  WARNING: This will CLEAR corrupted sessions for ALL companies. Continue? (yes/no): ',
        resolve
      );
    });

    if (confirmAnswer.toLowerCase() !== 'yes') {
      console.log('Migration cancelled.');
      rl.close();
      process.exit(0);
    }
  }

  // Step 4: Migrate company by company
  console.log('\n=== STARTING MIGRATION ===\n');

  for (const report of sessionsByCompany.values()) {
    if (report.corruptedSessions > 0) {
      await migrateCompany(report, dryRun);

      // Rate limit: Wait 5 seconds between companies
      if (!dryRun) {
        console.log('â³ Waiting 5 seconds before next company...\n');
        await new Promise(resolve => setTimeout(resolve, 5000));
      }
    }
  }

  console.log('=== MIGRATION COMPLETE ===\n');

  if (dryRun) {
    console.log('This was a DRY RUN. No changes were made.');
  } else {
    console.log('âœ… Corrupted sessions have been migrated.');
    console.log('\nğŸ“§ Next steps:');
    console.log('  1. Review migration-log.json for details');
    console.log('  2. Notify companies about re-pairing requirements');
    console.log('  3. Monitor new device IDs after re-pairing');
  }

  rl.close();
}

main().catch(console.error);
```

**Benefits:**
- âœ… Company-scoped migration
- âœ… Rate limiting between companies
- âœ… Detailed per-company logging
- âœ… Can notify specific companies
- âœ… Handles orphaned sessions
- âœ… Dry run mode

---

##### 4. Server-Sent Events (SSE) for QR

**Original Proposal:**
```typescript
export async function getQRCodeStream(req: Request, res: Response) {
  res.writeHead(200, {
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache',
    'Connection': 'keep-alive',
  });

  await connectionManager.createConnectionWithQRCallback(
    phoneNumber,
    (qr: string) => {
      res.write(`data: ${JSON.stringify({ qr })}\n\n`);
    },
    (status: string) => {
      res.write(`data: ${JSON.stringify({ status })}\n\n`);
      if (status === 'open') {
        res.end();
      }
    }
  );
}
```

**Multi-Tenant Concerns:**

âŒ **SSE Connection Limits:**
- Each SSE keeps HTTP connection open
- With 50 companies pairing simultaneously: 50 open connections
- Node.js default: 1024 max file descriptors
- Each connection: 1 FD + ~512KB memory

âŒ **No Timeout:**
- SSE connection stays open until status === 'open'
- If pairing never completes: connection leaks
- With many companies: gradual resource exhaustion

âŒ **No Per-Company Rate Limit:**
- One company could spam /qr-stream
- Open 100 connections rapidly
- Exhaust resources

**âœ… Improved Multi-Tenant SSE:**

```typescript
class SSEConnectionManager {
  private activeConnections = new Map<
    string,
    {
      res: Response;
      sessionPhone: string;
      companyId: string;
      startedAt: Date;
      lastActivity: Date;
    }
  >();

  private readonly MAX_CONNECTIONS_PER_COMPANY = 3;
  private readonly MAX_TOTAL_CONNECTIONS = 50;
  private readonly SSE_TIMEOUT_MS = 120000; // 2 minutes

  async createSSEConnection(
    req: Request,
    res: Response,
    sessionPhone: string,
    companyId: string
  ): Promise<void> {
    // Check company limit
    const companyConnections = this.getCompanyConnectionCount(companyId);
    if (companyConnections >= this.MAX_CONNECTIONS_PER_COMPANY) {
      res.status(429).json({
        error: 'Too many concurrent QR requests for this company',
        limit: this.MAX_CONNECTIONS_PER_COMPANY,
        current: companyConnections,
      });
      return;
    }

    // Check global limit
    if (this.activeConnections.size >= this.MAX_TOTAL_CONNECTIONS) {
      res.status(503).json({
        error: 'Server at capacity, please try again later',
        limit: this.MAX_TOTAL_CONNECTIONS,
      });
      return;
    }

    // Generate connection ID
    const connectionId = `${sessionPhone}-${Date.now()}`;

    // Setup SSE
    res.writeHead(200, {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
      'X-Accel-Buffering': 'no', // Disable nginx buffering
    });

    // Send keep-alive ping every 30s
    const keepAliveInterval = setInterval(() => {
      res.write(':ping\n\n');
      this.updateActivity(connectionId);
    }, 30000);

    // Timeout
    const timeout = setTimeout(() => {
      this.sendEvent(res, 'error', {
        message: 'QR generation timeout',
        code: 'TIMEOUT',
      });
      this.closeConnection(connectionId, keepAliveInterval);
    }, this.SSE_TIMEOUT_MS);

    // Track connection
    this.activeConnections.set(connectionId, {
      res,
      sessionPhone,
      companyId,
      startedAt: new Date(),
      lastActivity: new Date(),
    });

    // Cleanup on client disconnect
    req.on('close', () => {
      clearInterval(keepAliveInterval);
      clearTimeout(timeout);
      this.activeConnections.delete(connectionId);
      logger.info(
        `[${companyId}] SSE connection closed for ${sessionPhone}`
      );
    });

    // Create connection with callbacks
    try {
      await connectionManager.createConnectionWithQRCallback(
        sessionPhone,
        companyId,
        (qr: string) => {
          this.sendEvent(res, 'qr', { qr });
          this.updateActivity(connectionId);
        },
        (status: string) => {
          this.sendEvent(res, 'status', { status });
          this.updateActivity(connectionId);

          if (status === 'open') {
            clearInterval(keepAliveInterval);
            clearTimeout(timeout);
            this.sendEvent(res, 'complete', {
              message: 'Connection established'
            });
            this.closeConnection(connectionId, keepAliveInterval);
          }
        }
      );
    } catch (error) {
      clearInterval(keepAliveInterval);
      clearTimeout(timeout);
      this.sendEvent(res, 'error', {
        message: String(error),
        code: 'CONNECTION_ERROR',
      });
      this.closeConnection(connectionId, keepAliveInterval);
    }
  }

  private sendEvent(res: Response, event: string, data: any): void {
    try {
      res.write(`event: ${event}\n`);
      res.write(`data: ${JSON.stringify(data)}\n\n`);
    } catch (error) {
      logger.error('Failed to send SSE event:', error);
    }
  }

  private updateActivity(connectionId: string): void {
    const conn = this.activeConnections.get(connectionId);
    if (conn) {
      conn.lastActivity = new Date();
    }
  }

  private closeConnection(
    connectionId: string,
    interval?: NodeJS.Timeout
  ): void {
    const conn = this.activeConnections.get(connectionId);
    if (conn) {
      try {
        conn.res.end();
      } catch (error) {
        // Already closed
      }
      this.activeConnections.delete(connectionId);
    }
    if (interval) {
      clearInterval(interval);
    }
  }

  private getCompanyConnectionCount(companyId: string): number {
    let count = 0;
    for (const conn of this.activeConnections.values()) {
      if (conn.companyId === companyId) {
        count++;
      }
    }
    return count;
  }

  // Cleanup stale connections
  cleanupStaleConnections(): void {
    const now = Date.now();
    const staleThreshold = this.SSE_TIMEOUT_MS;

    for (const [connId, conn] of this.activeConnections.entries()) {
      const age = now - conn.lastActivity.getTime();
      if (age > staleThreshold) {
        logger.warn(
          `Closing stale SSE connection for ${conn.sessionPhone} ` +
          `(age: ${Math.round(age / 1000)}s)`
        );
        this.closeConnection(connId);
      }
    }
  }

  getStats() {
    const stats = {
      totalConnections: this.activeConnections.size,
      byCompany: new Map<string, number>(),
    };

    for (const conn of this.activeConnections.values()) {
      const count = stats.byCompany.get(conn.companyId) || 0;
      stats.byCompany.set(conn.companyId, count + 1);
    }

    return stats;
  }
}

// Singleton
const sseManager = new SSEConnectionManager();

// Cleanup every minute
setInterval(() => {
  sseManager.cleanupStaleConnections();
}, 60000);

export default sseManager;
```

**Benefits:**
- âœ… Per-company connection limits
- âœ… Global connection cap
- âœ… Automatic timeout and cleanup
- âœ… Monitoring and stats
- âœ… Keep-alive ping
- âœ… Graceful error handling

---

### Option B: Incremental Fix

**Proposed Changes:**
1. Add device ID validation on connection
2. Extend inspectAuthState() to check device ID
3. Add health check endpoint

#### Multi-Tenant Impact Assessment

##### Health Check Endpoint

**Original Proposal:**
```typescript
export async function getSessionHealth(req: Request, res: Response) {
  const { phoneNumber } = req.params;
  const socket = connectionManager.getConnection(phoneNumber);

  if (!socket || !socket.user) {
    return res.json({
      healthy: false,
      issue: 'socket.user not initialized',
    });
  }

  const deviceId = extractDeviceId(socket.user.id);
  const healthy = deviceId >= 0 && deviceId <= 10;

  res.json({
    healthy,
    deviceId,
    userId: socket.user.id,
  });
}
```

**Multi-Tenant Concerns:**

âŒ **No Company Context:**
- Endpoint doesn't require authentication
- Anyone can check any session's health
- Potential information disclosure

âŒ **No Rate Limiting:**
- Can be spammed for all sessions
- Discover which phone numbers are active

**âœ… Improved Multi-Tenant Health Check:**

```typescript
// Requires authentication
export async function getSessionHealth(
  req: Request,
  res: Response,
  next: NextFunction
) {
  try {
    const { phoneNumber } = req.params;
    const companyId = req.companyId; // From requireTenant middleware

    // Verify this company owns this session
    const sessionOwner = await getSessionOwner(phoneNumber);

    if (!sessionOwner) {
      return res.status(404).json({
        error: 'Session not found',
      });
    }

    if (sessionOwner !== companyId) {
      return res.status(403).json({
        error: 'Access denied - session belongs to different company',
      });
    }

    // Get socket
    const socket = connectionManager.getConnection(phoneNumber);

    if (!socket || !socket.user) {
      return res.json({
        healthy: false,
        issue: 'socket.user not initialized',
        recommendation: 're-pair required',
        sessionOwner: companyId,
      });
    }

    // Extract device ID
    const deviceIdMatch = socket.user.id.match(/:(\d+)@/);
    const deviceId = deviceIdMatch ? parseInt(deviceIdMatch[1], 10) : -1;
    const healthy = deviceId >= 0 && deviceId <= 10;

    // Response
    res.json({
      healthy,
      deviceId,
      userId: socket.user.id,
      issue: healthy ? null : 'abnormal device ID',
      recommendation: healthy ? null : 're-pair required',
      sessionOwner: companyId,
      connectedAt: socket.connectedAt,
      lastActivity: socket.lastActivity,
    });
  } catch (error) {
    next(error);
  }
}

// Helper: Get session owner from mapping
async function getSessionOwner(phoneNumber: string): Promise<string | null> {
  // Implementation depends on how sessions are linked to companies
  // Option 1: Query database
  // Option 2: Session metadata file
  // Option 3: API usage logs

  return null; // Placeholder
}
```

**Benefits:**
- âœ… Authentication required
- âœ… Company ownership validation
- âœ… Information isolation
- âœ… Detailed response

---

### Option C: Hybrid Approach

**Already covered** - Combines Option A (long-term) + Option B (short-term)

**Multi-Tenant Considerations:**
- Phase 1 (Option B improvements) must include company scoping
- Phase 2 (Option A refactoring) must use multi-tenant implementations shown above

---

## ğŸ”¥ CRITICAL SCALABILITY ISSUES

### Issue 1: Memory Exhaustion with 50 Sessions

**Current State:**
- ~100 MB per active session
- 50 sessions = ~5 GB required
- Node.js default heap: 4 GB
- **Result:** Out of memory crashes

**Impact on Multi-Tenant:**
- All companies affected when instance crashes
- No graceful degradation
- No per-company limits

**Solution:**

```typescript
class SessionResourceManager {
  private readonly MAX_MEMORY_MB = 3500; // Leave 500MB buffer
  private readonly MB_PER_SESSION = 100;
  private readonly MAX_SESSIONS = Math.floor(this.MAX_MEMORY_MB / this.MB_PER_SESSION); // 35

  async checkResourceAvailability(companyId: string): Promise<boolean> {
    const currentSessions = connectionManager.getAllConnections().size;

    if (currentSessions >= this.MAX_SESSIONS) {
      logger.warn(
        `[${companyId}] Session creation blocked: at maximum capacity ` +
        `(${currentSessions}/${this.MAX_SESSIONS})`
      );
      return false;
    }

    // Check memory usage
    const memUsage = process.memoryUsage();
    const heapUsedMB = memUsage.heapUsed / 1024 / 1024;

    if (heapUsedMB > this.MAX_MEMORY_MB) {
      logger.error(
        `[${companyId}] Session creation blocked: memory limit exceeded ` +
        `(${heapUsedMB.toFixed(2)} MB / ${this.MAX_MEMORY_MB} MB)`
      );
      return false;
    }

    return true;
  }

  async createSessionWithResourceCheck(
    sessionPhone: string,
    companyId: string
  ): Promise<any> {
    const available = await this.checkResourceAvailability(companyId);

    if (!available) {
      throw new Error(
        'Server at capacity. Please try again later or contact support.'
      );
    }

    return await connectionManager.createConnection(sessionPhone);
  }

  // Periodic cleanup of inactive sessions
  async enforceResourceLimits(): Promise<void> {
    const currentSessions = connectionManager.getAllConnections().size;

    if (currentSessions < this.MAX_SESSIONS) {
      return; // Under limit
    }

    logger.warn(
      `Resource limit enforcement: ${currentSessions}/${this.MAX_SESSIONS} sessions`
    );

    // Find sessions to evict (oldest, least active)
    const sessions = Array.from(connectionManager.getAllConnections().entries());

    // Sort by last activity (ascending)
    sessions.sort((a, b) => {
      const aActivity = a[1].lastActivity || 0;
      const bActivity = b[1].lastActivity || 0;
      return aActivity - bActivity;
    });

    // Disconnect oldest 10% if over limit
    const toDisconnect = Math.ceil((currentSessions - this.MAX_SESSIONS) * 1.1);

    for (let i = 0; i < toDisconnect && i < sessions.length; i++) {
      const [sessionPhone, socket] = sessions[i];
      logger.info(`Disconnecting inactive session: ${sessionPhone}`);
      await connectionManager.disconnect(sessionPhone);
    }
  }
}

const resourceManager = new SessionResourceManager();

// Check every 5 minutes
setInterval(() => {
  resourceManager.enforceResourceLimits().catch(error => {
    logger.error('Failed to enforce resource limits:', error);
  });
}, 300000);

export default resourceManager;
```

---

### Issue 2: MongoDB Connection Pool Saturation

**Current State:**
- 5 MongoDB connections shared by all tenants
- Every quota check uses a connection
- 30 messages/minute per company Ã— 10 companies = 300 quota checks/minute
- **Result:** Connection pool saturated, delays quota checks

**Solution:**

```typescript
// In-memory quota cache with TTL
class QuotaCacheManager {
  private cache = new Map<
    string,
    {
      value: any;
      fetchedAt: number;
      expiresAt: number;
    }
  >();

  private readonly CACHE_TTL_MS = 60000; // 1 minute

  async getOrFetch<T>(
    key: string,
    fetcher: () => Promise<T>
  ): Promise<T> {
    const cached = this.cache.get(key);

    if (cached && Date.now() < cached.expiresAt) {
      return cached.value as T;
    }

    // Fetch fresh
    const value = await fetcher();
    const now = Date.now();

    this.cache.set(key, {
      value,
      fetchedAt: now,
      expiresAt: now + this.CACHE_TTL_MS,
    });

    return value;
  }

  invalidate(key: string): void {
    this.cache.delete(key);
  }

  // Cleanup expired entries
  cleanup(): void {
    const now = Date.now();
    for (const [key, entry] of this.cache.entries()) {
      if (now >= entry.expiresAt) {
        this.cache.delete(key);
      }
    }
  }
}

const quotaCache = new QuotaCacheManager();

// Cleanup every 2 minutes
setInterval(() => {
  quotaCache.cleanup();
}, 120000);

// Usage in quota middleware
export async function requireWhatsAppQuota(
  req: Request,
  res: Response,
  next: NextFunction
) {
  const companyId = req.companyId;

  // Cache key
  const cacheKey = `whatsapp_quota:${companyId}`;

  try {
    const quota = await quotaCache.getOrFetch(cacheKey, async () => {
      return await quotaValidatorService.checkWhatsAppQuota(companyId);
    });

    if (!quota.allowed) {
      return res.status(429).json({
        error: 'WhatsApp quota exceeded',
        quota: quota.details,
      });
    }

    next();
  } catch (error) {
    // On cache/DB error, allow request (fail open)
    logger.error(`Quota check failed for ${companyId}:`, error);
    next();
  }
}
```

**Benefits:**
- âœ… Reduces MongoDB queries by ~90%
- âœ… 1-minute stale data acceptable for quotas
- âœ… Fail-open on errors (better UX)
- âœ… Automatic cache invalidation

---

### Issue 3: Rate Limiter Not Distributed

**Current State:**
- In-memory Map for rate limits
- Single instance only
- Limits reset on restart

**Solution:**

```typescript
// Option 1: Redis-based rate limiter (preferred for multi-instance)
import Redis from 'ioredis';

class RedisRateLimiter {
  private redis: Redis;

  constructor(redisUrl: string) {
    this.redis = new Redis(redisUrl);
  }

  async checkLimit(
    key: string,
    limit: number,
    windowMs: number
  ): Promise<{ allowed: boolean; remaining: number; reset: number }> {
    const now = Date.now();
    const windowStart = now - windowMs;

    // Use Redis sorted set to track requests in time window
    const multi = this.redis.multi();

    // Remove old entries
    multi.zremrangebyscore(key, '-inf', windowStart);

    // Count current requests
    multi.zcard(key);

    // Add current request
    multi.zadd(key, now, `${now}-${Math.random()}`);

    // Set expiry
    multi.expire(key, Math.ceil(windowMs / 1000));

    const results = await multi.exec();

    const count = results[1][1] as number;
    const remaining = Math.max(0, limit - count - 1);
    const allowed = count < limit;
    const reset = now + windowMs;

    return { allowed, remaining, reset };
  }
}

// Option 2: Hybrid (in-memory + Redis sync)
class HybridRateLimiter {
  private localCache = new Map<string, { count: number; resetAt: number }>();
  private redis: Redis;

  async checkLimit(
    key: string,
    limit: number,
    windowMs: number
  ): Promise<{ allowed: boolean; remaining: number }> {
    // Check local cache first (fast)
    const local = this.localCache.get(key);
    const now = Date.now();

    if (local && now < local.resetAt) {
      if (local.count >= limit) {
        return { allowed: false, remaining: 0 };
      }
      local.count++;
      return { allowed: true, remaining: limit - local.count };
    }

    // Sync with Redis (slower, but authoritative)
    const redisResult = await this.redis.incr(key);

    if (redisResult === 1) {
      await this.redis.pexpire(key, windowMs);
    }

    const allowed = redisResult <= limit;
    const remaining = Math.max(0, limit - redisResult);

    // Update local cache
    this.localCache.set(key, {
      count: redisResult,
      resetAt: now + windowMs,
    });

    return { allowed, remaining };
  }
}
```

**For Current Setup (No Redis):**

Keep in-memory but add persistence:

```typescript
class PersistentRateLimiter {
  private limitsFile = './data/rate-limits.json';
  private limits = new Map<string, RateLimitRecord>();
  private saveTimer: NodeJS.Timeout;

  constructor() {
    this.loadFromDisk();

    // Save every 30 seconds
    this.saveTimer = setInterval(() => {
      this.saveToDisk();
    }, 30000);
  }

  private async loadFromDisk(): Promise<void> {
    if (await fs.pathExists(this.limitsFile)) {
      try {
        const data = await fs.readJson(this.limitsFile);
        this.limits = new Map(Object.entries(data));
        logger.info(`Loaded ${this.limits.size} rate limit entries from disk`);
      } catch (error) {
        logger.error('Failed to load rate limits:', error);
      }
    }
  }

  private async saveToDisk(): Promise<void> {
    try {
      const data = Object.fromEntries(this.limits.entries());
      await fs.writeJson(this.limitsFile, data);
      logger.debug(`Saved ${this.limits.size} rate limit entries to disk`);
    } catch (error) {
      logger.error('Failed to save rate limits:', error);
    }
  }

  async shutdown(): Promise<void> {
    clearInterval(this.saveTimer);
    await this.saveToDisk();
  }
}
```

---

## ğŸ“ˆ RECOMMENDED ARCHITECTURE FOR SCALE

### Phase 1: Current Instance Optimization (20-35 sessions)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               lila-app Instance (Optimized)             â”‚
â”‚                                                          â”‚
â”‚  âœ… Resource limits (35 sessions max)                   â”‚
â”‚  âœ… Quota caching (1-min TTL)                           â”‚
â”‚  âœ… Multi-tenant store manager                          â”‚
â”‚  âœ… Device ID validation                                â”‚
â”‚  âœ… Per-company rate limits                             â”‚
â”‚  âœ… Memory monitoring                                   â”‚
â”‚  âœ… Session eviction policy                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Estimated Capacity:**
- 10-15 companies
- 20-35 concurrent sessions
- 30 messages/min per company
- ~300-500 messages/min total

---

### Phase 2: Horizontal Scaling (50-100 sessions)

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Load Balancer   â”‚
                â”‚   (Nginx/HAProxy)â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚  Instance 1 â”‚            â”‚  Instance 2 â”‚
    â”‚  (20 sess)  â”‚            â”‚  (20 sess)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚                           â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Redis Cluster  â”‚
                â”‚  - Rate limits  â”‚
                â”‚  - Quota cache  â”‚
                â”‚  - Session mgmt â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Requirements:**
- âœ… Redis for distributed rate limiting
- âœ… Sticky sessions (or session affinity)
- âœ… Shared MongoDB connection
- âœ… Health check endpoint for LB

**Estimated Capacity:**
- 20-30 companies
- 40-60 concurrent sessions
- 60 messages/min per company
- ~1200-1800 messages/min total

---

### Phase 3: Microservices Architecture (100+ sessions)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Portal    â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚   API GW     â”‚
â”‚  (Admin UI) â”‚       â”‚  (Auth/Route)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚               â”‚               â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
       â”‚  lila-app   â”‚ â”‚ WhatsApp   â”‚ â”‚  Storage   â”‚
       â”‚  (REST API) â”‚ â”‚  Service   â”‚ â”‚  Service   â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
              â”‚               â”‚               â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ Message Queue â”‚
                      â”‚  (RabbitMQ)   â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- âœ… Dedicated WhatsApp connection service
- âœ… Async message processing via queue
- âœ… Independent scaling per service
- âœ… Fault isolation

---

## ğŸ¯ ACTIONABLE RECOMMENDATIONS

### Immediate (This Week)

1. **Add Resource Limits**
   ```typescript
   MAX_SESSIONS = 35 (leave buffer for 4GB heap)
   MAX_MEMORY_MB = 3500
   EVICTION_POLICY = 'least-recently-used'
   ```

2. **Implement Quota Caching**
   ```typescript
   CACHE_TTL = 60 seconds
   FAIL_MODE = 'open' (allow on error)
   ```

3. **Add Company Scoping to Migration**
   - Use multi-tenant migration script
   - Track migrations per company
   - Rate limit: 5 seconds between companies

4. **Validate Device ID with Company Context**
   - Log company ID with every validation
   - Track validation failures per company
   - Alert on company-wide issues

### Short-Term (Next Sprint)

5. **Implement Multi-Tenant Store Manager**
   - Per-company store limits
   - Automatic cleanup of inactive stores
   - Size limits per store

6. **Add Monitoring**
   ```typescript
   Metrics:
   - Active sessions per company
   - Memory usage per session
   - Device ID distribution
   - Validation failures per company
   - Message throughput per company
   ```

7. **Improve SSE (if implementing)**
   - Per-company connection limits
   - Global connection cap
   - Automatic timeout and cleanup

### Medium-Term (Next Quarter)

8. **Deploy Redis**
   - Distributed rate limiting
   - Shared quota cache
   - Session metadata

9. **Horizontal Scaling**
   - Add second instance
   - Setup load balancer
   - Test failover

10. **Add Capacity Planning**
    - Monitor growth trends
    - Project capacity needs
    - Plan scaling triggers

---

## ğŸš¨ RISK MATRIX

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| **Out of Memory (35+ sessions)** | High | Critical | Resource limits + eviction |
| **MongoDB pool saturation** | Medium | High | Quota caching |
| **Single instance failure** | Medium | Critical | PM2 auto-restart + monitoring |
| **Cross-tenant interference** | Low | High | Company-scoped validation |
| **Migration storms** | Medium | Medium | Rate limiting between companies |
| **SSE connection exhaustion** | Medium | Medium | Per-company limits + timeout |

---

## ğŸ“Š TESTING CHECKLIST

### Multi-Tenant Scenarios

- [ ] **10 companies, 2 sessions each (20 total)**
  - All sessions connect successfully
  - No cross-tenant interference
  - Device IDs remain valid

- [ ] **5 companies, 5 sessions each (25 total)**
  - Resource manager accepts all
  - No memory issues
  - Quota checks performant

- [ ] **Reach capacity (35 sessions)**
  - 36th session rejected gracefully
  - Existing sessions unaffected
  - Clear error message to user

- [ ] **Migration with 5 companies**
  - Each company migrated separately
  - 5-second delay between companies
  - Per-company logging works

- [ ] **Quota exhaustion for one company**
  - Only that company blocked
  - Other companies unaffected
  - Proper 429 response

- [ ] **One company's session corruption**
  - Auto-cleared without affecting others
  - Company-specific logging
  - Other sessions stable

---

## âœ… ACCEPTANCE CRITERIA

The solution is **multi-tenant ready and scalable** when:

1. âœ… **Resource limits enforced**
   - Maximum 35 sessions per instance
   - Graceful rejection when at capacity
   - Automatic eviction of inactive sessions

2. âœ… **Company isolation**
   - Validation scoped per company
   - Logging includes company context
   - No cross-tenant data leakage

3. âœ… **MongoDB optimized**
   - Quota caching reduces queries by 90%
   - Connection pool not saturated
   - < 50ms p99 for quota checks

4. âœ… **Monitoring in place**
   - Sessions per company tracked
   - Memory usage monitored
   - Device ID distribution logged
   - Alerts on anomalies

5. âœ… **Migration tested**
   - Works with multiple companies
   - Rate limited between companies
   - Per-company rollback possible

6. âœ… **Documentation updated**
   - Multi-tenant considerations documented
   - Capacity planning guide written
   - Runbook for scaling scenarios

---

## ğŸ“ CONCLUSION

The original `DEVICE_ID_ROOT_CAUSE_ANALYSIS.md` provides excellent technical insight into the root cause, but **lacks multi-tenant awareness**.

This document bridges that gap by:

1. âœ… Analyzing multi-tenant architecture
2. âœ… Identifying scalability bottlenecks
3. âœ… Proposing multi-tenant-safe implementations
4. âœ… Defining resource limits and eviction policies
5. âœ… Providing company-scoped migration strategy
6. âœ… Outlining path from 20 â†’ 100+ sessions

**Key Takeaway:** All proposed solutions (Options A, B, C) are viable but **MUST incorporate multi-tenant considerations** from this document to be production-ready for lila-app's shared-instance architecture.

---

**Document Status:** âœ… Complete
**Next Action:** Review with team â†’ Approve combined approach â†’ Begin implementation
**Priority:** ğŸ”´ Critical - Multi-tenant safety is non-negotiable
